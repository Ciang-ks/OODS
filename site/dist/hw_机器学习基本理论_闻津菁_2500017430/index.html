<!DOCTYPE html><html lang="zh-CN"> <head><meta charset="UTF-8"><meta name="description" content="OODS Homeworks"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><meta name="generator" content="Astro v4.16.19"><title>机器学习基本理论简介</title><meta name="astro-view-transitions-enabled" content="true"><meta name="astro-view-transitions-fallback" content="animate"><link rel="stylesheet" href="/OODS/_astro/_slug_.yDLEMZEj.css"><script type="module" src="/OODS/_astro/hoisted.DVZEIXQB.js"></script></head> <body class="bg-white text-slate-900 min-h-screen selection:bg-indigo-100 selection:text-indigo-900"> <header class="bg-[#24292e] text-white top-0 sticky z-50 shadow-sm"> <div class="max-w-4xl mx-auto px-6 h-16 flex items-center justify-between"> <a href="/OODS" class="font-bold text-xl tracking-tight text-white/90 hover:text-white transition-colors flex items-center gap-2"> <svg height="20" viewBox="0 0 24 24" width="20" fill="currentColor" style="opacity: 0.8;"><path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zm-1-13h2v6h-2zm0 8h2v2h-2z"></path></svg>
OODS
</a> <span class="text-xs font-mono text-white/60">Course Works</span> </div> </header> <main class="max-w-4xl mx-auto px-6 py-12">  <article class="max-w-none"> <header class="mb-16 pb-12 border-b border-slate-200 relative -mx-6 px-6 py-16 md:py-24 overflow-hidden" style="background-image: url('/OODS/machine_learning.png'); background-size: cover; background-position: center;"> <div class="absolute inset-0 bg-gradient-to-b from-white/90 via-white/85 to-white/95 backdrop-blur-[2px]"></div> <div class="relative z-10"> <div class="flex flex-wrap items-center gap-3 text-sm mb-6 font-mono"> <span class="uppercase tracking-wider text-xs font-semibold text-slate-600">report</span> </div> <h1 class="text-4xl md:text-5xl font-extrabold mb-8 leading-tight tracking-tight text-balance text-slate-900"> 机器学习基本理论简介 </h1> <div class="flex flex-wrap gap-4"> <div class="flex items-center gap-3 bg-white/90 backdrop-blur-sm pr-4 pl-2 py-2 rounded-full border border-slate-200 shadow-sm"> <div class="w-8 h-8 rounded-full bg-slate-100 flex items-center justify-center text-xs font-bold text-slate-600 border border-slate-200"> 闻 </div> <span class="text-base font-medium text-slate-700"> 闻津菁 </span> </div> </div> </div> </header> <div class="content-body relative markdown-body" style="background-color: transparent;"> <h2 id="深度学习理论的启示与前瞻">深度学习理论的启示与前瞻</h2>
<p>本次课程不仅对关于数据科学与人工智能的知识技术进行了复盘，更对科学范式以及其未来发展方向进行了深度探讨</p>
<h2 id="核心反思理论与实践的断裂与共生">核心反思：理论与实践的“断裂”与“共生”</h2>
<p>数据科学或人工智能行业的从业者始终面临一个尴尬的事实，即大家都认可深度学习方法的的效力，但其具体运作方式至今仍然是一个“黑箱”，没有人知道它到底为什么能产生如此好的效果。这说明了深度学习仍处于一种原始阶段，也未来其理论的研究方向。</p>
<p>而我们回顾机器学习理论的发展：学习的本质是通过获得多组多维数据x和y来推导出x到y的映射关系f，而得到f的方法即是计算f(x)与y之间的误差并取误差最小的一个f。这样一来，学习问题就被转化成了寻找最小值的问题。传统路径选择使用多项式逼近f，数学理论告诉我们，只要有足够的数据和合适的模型，我们就能逼近任何函数到任意精度。这为学习提供了理论上的“安全感”。但与此同时，多项式逼近也带来了“维数灾难”，使得逼近效率在维数较大时极为低下。</p>
<p>然而，深度学习即神经网络的实践却走上了一条看似“离经叛道”的道路，不再仅仅依赖于多项式逼近这样安全而稳妥的方法。深度学习的方法定义了与多维向量x对应的代表“权重”的多维向量w，根据计算x·w+b与0的大小关系达成“判断”的效果，由此一来便把学习的问题转化为了优化w和b的问题。而这样的优化可以通过一种被数学证明一定收敛的方法——随机梯度下降法—— 来实现。这种转向提供了比多项式逼近更高效的优化方案，打破了“维数灾难”的限制。</p>
<h2 id="历史回眸从寒冬到盛夏的三次浪潮">历史回眸：从“寒冬”到“盛夏”的三次浪潮</h2>
<p>事实上，神经网络的方法早在人工智能兴起的初期就已经被提出，只是当时并没有被采纳。回顾人工智能发展历史，大致有以下两次兴衰：</p>
<p>人工智能的第一次热潮发生在1957年，Rosenblatt发明感知机，即为单层神经网络。但Minsky指出其无法解决非线性函数<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup>，如简单的“亦或”计算，这说明了该模型的表达能力不足，并导致了第一个AI寒冬。此时，理论走到了实践前面，并“证明”了实践的失败。</p>
<p>多层神经网络和反向传播算法的提出，带来了第二次热潮。“万能逼近定理”证明神经网络可以逼近所有函数<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup>，这在理论上赋予了神经网络“无所不能”的潜力。然而，实践很快遇到了新的困难：理论研究者证明，多层神经网络（在特定初始值时）在找损失函数全局极小值困难，复杂的非线性与非凸性使随机梯度下降时容易被局部极小值困住<sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup>，这说明了此时模型的表达能力未被释放。</p>
<p>而我们正身处第三次浪潮之中。这次的爆发并非源于理论的根本性突破，而是由三大工程性因素驱动：首先是数据：互联网提供了从前难以获取的大规模数据。其次是算力：强大的GPU集群是数据得以被利用。最后是技术技巧：第三次热潮中的算法采用了更优的初始化条件，使得优化不再被局部极小点困住，打破了第二次寒冬时理论工作者的悲观预言。除此以外，第三次热潮的爆发还离不开在前三者支持下被发现的Scaling law，人们发现给模型提供的数据越多，模型的性能越好，且这种正相关没有边际效益，这样明确无风险的收益吸引到了大量的投资，这才有了ChatGPT的诞生。</p>
<h2 id="专题拓展破解当代深度学习的未解之谜">专题拓展：破解当代深度学习的“未解之谜”</h2>
<p>课程的后半部分，提出了深度学习理论目前的未解之谜以及研究方向。</p>
<p>我们至今仍不知道为什么神经网络的优化过程能达到如此好的效果。神经网络的优化过程类似一个黑箱，我们只知道它能够绕过局部极小点到最小值，却不知道达成这个结果的具体原理。其中一种猜测认为局部极小值可能并非想象中那么多，大部分驻点可能是“鞍点”，而梯度下降法有能力逃离鞍点。<sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup>另一种猜测认为目标函数没有局部极小点。此外，Meta的研究表明，优化有时仅依赖极少数参数的改变，这样优化过程自然变得更容易。<sup><a href="#user-content-fn-5" id="user-content-fnref-5" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup></p>
<p>另一个广受关注的问题是模型基于少量样本的训练结果为什么能泛化推广。当模型参数数量超出训练样本数时，模型完全有能力“记住”所有数据，而不是学习其规律。根据经典统计学习理论，随模型复杂度增加，泛化误差会先下降后上升，形成U形曲线，参数量过大必然导致严重的过拟合。U形曲线说明模型首先从欠拟合状态逐渐通过增加参数、提高表达能力达到完全拟合，而后开始受到噪声的干扰，即过拟合，由于“死记硬背”噪声的影响而失去了泛化能力。但事实是，这些大模型展现出了惊人的泛化能力，泛化误差出现了“双下降”现象<sup><a href="#user-content-fn-6" id="user-content-fnref-6" data-footnote-ref="" aria-describedby="footnote-label">6</a></sup>：研究发现，在经历U形曲线后，如果模型复杂度继续增大，泛化误差会再次下降。随着参数量进一步增大，优化算法在噪声干扰中抽离出普遍真理，优先选择了简单的解释，模型逐渐从僵硬变得灵活。有研究者认为优化算法本身具有偏好，它倾向于优先寻找“简单”的解，而不是死记硬背所有的数据，进而获得了泛化能力。随之而来的问题便是：为什么优化算法会有对“简单”解的偏好？这可能源于随机梯度下降过程中对“平滑”的偏好<sup><a href="#user-content-fn-7" id="user-content-fnref-7" data-footnote-ref="" aria-describedby="footnote-label">7</a></sup>：过拟合的解在函数空间中往往是陡峭的，这样在下降的过程中更容易被抛离这一区域。这使得下降的结果更容易落在“平滑的”即简单的解上。</p>
<h2 id="总结">总结</h2>
<p>我们对大模型的工作原理仍处于“盲人摸象”的阶段。未来的研究方向，必将聚焦于破解模型“黑箱”，寻找其理论本质。当这些问题被解开时，我们迎来的将不仅是更高效、更强大的模型，更是人工智能作为一种科学范式的真正成熟。</p>
<h2 id="ai协作记录">AI协作记录</h2>
<div style="font-size: small;">
<p><strong>Prompt 1:</strong>
具体解释一下“双下降”现象，复杂度是指数据量吗</p>
<p><strong>Answer:</strong>
deepseek回复：
复杂度是指数据量吗？答案是：不完全是。在“双下降”现象中，“复杂度”更准确地是指模型的容量或能力，通常由模型的参数量来衡量。而数据量（训练样本的数量）是另一个独立的、至关重要的因素。</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>“双下降”现象的详细解释：</span></span>
<span class="line"><span>第一阶段：经典U形曲线（欠参数化区域 → 临界点）</span></span>
<span class="line"><span>*   **欠参数化区域**：当模型参数很少（复杂度低）时，模型过于简单，无法捕捉数据中的基本规律。此时无论是训练误差还是测试（泛化）误差都很高，这就是 **“欠拟合”**。</span></span>
<span class="line"><span>*   **走向临界点**：随着模型参数量的增加，模型能力变强，开始学习到有效的规律。训练误差和测试误差都会下降，泛化性能提升。</span></span>
<span class="line"><span>第二阶段：临界点——插值阈值</span></span>
<span class="line"><span>这是一个非常关键的转折点。当模型复杂度刚刚达到能够**完美拟合（甚至记忆）所有训练数据**时，即训练误差降至几乎为0。</span></span>
<span class="line"><span>*   在经典理论中，这里被认为是**最危险**的区域。因为模型为了拟合所有点，包括噪声点，会变得极其“敏感”和“扭曲”，导致**严重的过拟合**，泛化误差会达到峰值。</span></span>
<span class="line"><span>*   此时，模型处于一种“僵硬的”状态，它没有多余的能力来以更平滑的方式拟合数据。</span></span>
<span class="line"><span>第三阶段：双下降的出现（过参数化区域）</span></span>
<span class="line"><span>这是反直觉和革命性的发现。当模型的参数量继续增加，远超插值阈值后，泛化误差非但不会维持在高位，反而会再次下降！</span></span>
<span class="line"><span>为什么会发生第二次下降？</span></span>
<span class="line"><span>1.  更平滑的插值：一个拥有巨大容量的模型，有无数种方式可以完美拟合训练数据（即达到训练误差为0）。在所有这些解中，梯度下降等优化算法倾向于找到的那个解，往往是一个更平滑、更简单的函数。想象一下，用一支巨大的画笔（高复杂度）可以轻松地画出一条平滑的曲线穿过所有数据点；而用一支小画笔（临界点复杂度）则需要扭曲自己才能勉强穿过，导致线条崎岖不平。</span></span>
<span class="line"><span>2.  隐式正则化：如你之前的笔记所探讨，优化过程本身存在一种隐式偏好（例如通过SGD算法），它会从所有能完美拟合训练数据的解中，挑选出那个泛化能力最好的解。在过参数化区域，这种简单的、泛化能力好的解是大量存在的。</span></span>
<span class="line"><span>3.  理论上的解释：某些简化模型（如线性回归）的理论分析证明，当参数无限多时，模型会收敛到使用最小范数解（一种最“简单”的解），而这个解通常具有良好的泛化性能。</span></span>
<span class="line"><span></span></span>
<span class="line"><span>“复杂度”与“数据量”的关系</span></span>
<span class="line"><span></span></span>
<span class="line"><span>   模型的复杂度：通常指模型的参数量。是模型固有的、描述其拟合能力上限的属性。</span></span>
<span class="line"><span>   数据量：指训练样本的数量（N）。</span></span>
<span class="line"><span></span></span>
<span class="line"><span>这两个因素共同决定了模型是否处于“过参数化”状态：</span></span>
<span class="line"><span>   过参数化 regime: 参数量 > 数据量。</span></span>
<span class="line"><span>   欠参数化 regime: 参数量 &#x3C; 数据量。</span></span>
<span class="line"><span>（有删减）</span></span>
<span class="line"><span></span></span></code></pre>
<p><strong>Prompt 2:</strong>
那么为什么优化算法对简单有偏好呢</p>
<p><strong>Answer:</strong>
deepseek回复：
以下是最主流的几种解释，它们共同构成了这个答案：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>1. 算法本身的“惰性”与平滑路径：SGD的噪声本质</span></span>
<span class="line"><span>随机梯度下降（SGD）是我们最常用的优化器，它的工作方式内在地倾向于简单解。</span></span>
<span class="line"><span>   比喻：想象你在下山，但每一步都蒙着眼睛，只根据脚下的一小块地方感觉坡度。在这种情况下，你会本能地倾向于走向宽阔、平缓的山谷，因为那里即使你的步子迈得不准，也不容易掉下悬崖。相反，如果你走向一个狭窄、陡峭的裂缝（一个复杂的、过拟合的解），任何一步的噪声和误差都可能让你撞上墙壁，甚至被“抛”出去。</span></span>
<span class="line"><span>   结论：SGD的噪声使得它在优化过程中对尖锐的、复杂的解非常不稳定，而平滑的、平坦的 minima（对应简单的解）则能更好地吸收这种更新噪声，因此更受SGD的“青睐”。研究表明，收敛于平坦区域的模型，其泛化能力显著更强。</span></span>
<span class="line"><span>（有删减）</span></span>
<span class="line"><span></span></span></code></pre>
<p><strong>Prompt 3:</strong>
你说训练在还没来得及记噪声是就终止，那么是不是在参数量大于样本量过拟合的情况下，损失函数随训练时间也形成U形曲线？</p>
<p><strong>Answer:</strong>
deepseek回复：
您的直觉是完全正确的。
第一阶段：曲线下降段（学习规律）
行为：模型正在快速学习训练数据中存在的普遍、稳健的规律。这些规律同样适用于未见过的测试数据。
结果：训练损失和测试损失同时稳步下降。模型的泛化能力随着训练不断增强。我们追求的目标，就是在此阶段找到最佳停止点。</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>第二阶段：曲线最低点（最佳拟合）</span></span>
<span class="line"><span>  此时模型已经较好地掌握了数据的核心规律，但还没有开始记忆噪声。</span></span>
<span class="line"><span></span></span>
<span class="line"><span>第三阶段：曲线上升段（记忆噪声）</span></span>
<span class="line"><span>  行为：模型已经学会了所有可用的通用规律。为了进一步最小化训练损失，它开始记忆训练集中的特定样本和随机噪声。</span></span>
<span class="line"><span>  结果：训练损失继续下降并趋近于零，然而测试损失开始反弹并上升，因为模型学到的这些“记忆”对于测试集来说是无用甚至有害的，这导致了过拟合。</span></span>
<span class="line"><span></span></span>
<span class="line"><span>与“双下降”现象的区别与联系</span></span>
<span class="line"><span></span></span>
<span class="line"><span>您可能会发现，这个U形曲线和之前讨论的“双下降”现象（模型复杂度与误差的关系）非常相似但又不同。它们是理解过参数化模型的两个互补视角：</span></span>
<span class="line"><span></span></span>
<span class="line"><span>一个生动的类比：</span></span>
<span class="line"><span></span></span>
<span class="line"><span>   “测试损失-U形曲线”好比观察一个学生准备一场考试。</span></span>
<span class="line"><span>      第一阶段：他学习通用的概念和解题原理。</span></span>
<span class="line"><span>      第二阶段：他达到了最佳状态，掌握了知识。</span></span>
<span class="line"><span>      第三阶段：他开始死记硬背特定习题的答案和印刷瑕疵，这反而损害了他解决新问题的能力。</span></span>
<span class="line"><span></span></span>
<span class="line"><span>   “双下降”现象 好比比较不同天赋的学生。</span></span>
<span class="line"><span>      能力弱的学生（欠参数化）：无法掌握核心知识。</span></span>
<span class="line"><span>      能力刚好的学生（临界点）：为了及格，扭曲地记忆所有东西，非常僵硬（过拟合峰值）。</span></span>
<span class="line"><span>      能力超强的学生（过参数化）：他完全可以轻松记忆所有细节，但他理解了更深层的原理，因此能用更优雅、更通用的方式解题（找到简单解）。</span></span>
<span class="line"><span>（有删减）</span></span>
<span class="line"><span></span></span></code></pre>
</div>
<section data-footnotes="" class="footnotes"><h2 class="sr-only" id="footnote-label">Footnotes</h2>
<ol>
<li id="user-content-fn-1">
<p>Minsky, M., &#x26; Papert, S. (1969). Perceptrons: An Introduction to Computational Geometry. MIT Press. <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-2">
<p>Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems. <a href="#user-content-fnref-2" data-footnote-backref="" aria-label="Back to reference 2" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-3">
<p>Gori, M., &#x26; Tesi, A. (1992). On the problem of local minima in backpropagation. IEEE Transactions on Pattern Analysis and Machine Intelligence <a href="#user-content-fnref-3" data-footnote-backref="" aria-label="Back to reference 3" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-4">
<p>Dauphin, Y., et al. (2014). Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. NeurIPS. <a href="#user-content-fnref-4" data-footnote-backref="" aria-label="Back to reference 4" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-5">
<p>Frankle, J., &#x26; Carbin, M. (2019). The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. ICLR. <a href="#user-content-fnref-5" data-footnote-backref="" aria-label="Back to reference 5" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-6">
<p>Belkin, M., et al. (2019). Reconciling modern machine learning practice and the bias-variance trade-off. PNAS. <a href="#user-content-fnref-6" data-footnote-backref="" aria-label="Back to reference 6" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-7">
<p>Neyshabur, B., et al. (2017). Exploring generalization in deep learning. NeurIPS. <a href="#user-content-fnref-7" data-footnote-backref="" aria-label="Back to reference 7" class="data-footnote-backref">↩</a></p>
</li>
</ol>
</section> </div> </article>   </main> <footer class="border-t border-slate-200 mt-20 py-12 text-center text-slate-500 text-sm"> <p>
&copy; 2026 OODS. All rights reserved.
</p> </footer> </body></html>