<!DOCTYPE html><html lang="zh-CN"> <head><meta charset="UTF-8"><meta name="description" content="OODS Homeworks"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><meta name="generator" content="Astro v4.16.19"><title>自然语言处理</title><meta name="astro-view-transitions-enabled" content="true"><meta name="astro-view-transitions-fallback" content="animate"><link rel="stylesheet" href="/OODS/_astro/_slug_.yDLEMZEj.css"><script type="module" src="/OODS/_astro/hoisted.DVZEIXQB.js"></script></head> <body class="bg-white text-slate-900 min-h-screen selection:bg-indigo-100 selection:text-indigo-900"> <header class="bg-[#24292e] text-white top-0 sticky z-50 shadow-sm"> <div class="max-w-4xl mx-auto px-6 h-16 flex items-center justify-between"> <a href="/OODS" class="font-bold text-xl tracking-tight text-white/90 hover:text-white transition-colors flex items-center gap-2"> <svg height="20" viewBox="0 0 24 24" width="20" fill="currentColor" style="opacity: 0.8;"><path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zm-1-13h2v6h-2zm0 8h2v2h-2z"></path></svg>
OODS
</a> <span class="text-xs font-mono text-white/60">Course Works</span> </div> </header> <main class="max-w-4xl mx-auto px-6 py-12">  <article class="max-w-none"> <header class="mb-16 pb-12 border-b border-slate-200 relative -mx-6 px-6 py-16 md:py-24 overflow-hidden" style="background-image: url('/OODS/NLP.png'); background-size: cover; background-position: center;"> <div class="absolute inset-0 bg-gradient-to-b from-white/90 via-white/85 to-white/95 backdrop-blur-[2px]"></div> <div class="relative z-10"> <div class="flex flex-wrap items-center gap-3 text-sm mb-6 font-mono"> <span class="uppercase tracking-wider text-xs font-semibold text-slate-600">report</span> </div> <h1 class="text-4xl md:text-5xl font-extrabold mb-8 leading-tight tracking-tight text-balance text-slate-900"> 自然语言处理 </h1> <div class="flex flex-wrap gap-4"> <div class="flex items-center gap-3 bg-white/90 backdrop-blur-sm pr-4 pl-2 py-2 rounded-full border border-slate-200 shadow-sm"> <div class="w-8 h-8 rounded-full bg-slate-100 flex items-center justify-center text-xs font-bold text-slate-600 border border-slate-200"> 郑 </div> <span class="text-base font-medium text-slate-700"> 郑喆琪 </span> </div> </div> </div> </header> <div class="content-body relative markdown-body" style="background-color: transparent;"> <h2 id="一自然语言处理的基本概念">一、自然语言处理的基本概念</h2>
<h3 id="1广义语言与自然语言">1.广义语言与自然语言</h3>
<ul>
<li>首先，我们需要明确自然语言处理的对象，即<strong>自然语言</strong>的内涵，并将其与<strong>广义语言</strong>进行辨析。</li>
<li><strong>自然语言</strong>是指随人类文化演化的自然形成的语言，不包括计算机语言和形式语言，具有高歧义、高上下文依赖的特征。</li>
<li><strong>广义语言</strong>是指一套共同采用的沟通符号、表达方式与处理原则，包括自然语言、动物信号、计算机语言等，具有多模态特征。</li>
</ul>
<h3 id="2自然语言处理nlp">2.自然语言处理(NLP)</h3>
<ul>
<li>NLP是指对人类特有的书面形式和口头形式的自然语言的信息进行各种类型处理和加工。</li>
<li>NLP的核心任务包括自然语言理解(NLU)和自然语言生成(NLG)。</li>
<li>NLU旨在从自然语言文本或语音中提取出确切的含义。典型的NLU任务包括词性标注、句法分析、语义角色标注、情感分析等。</li>
<li>NLG旨在让机器写出通顺、合理、符合语境的人类语言。典型NLG任务包括机器翻译、文本摘要、对话系统等</li>
<li>NLP一般采用多层处理结构，包括词法、句法、语义、语用四个层面</li>
</ul>
<h3 id="3自然语言处理的挑战">3.自然语言处理的挑战</h3>
<ul>
<li>NLP面临的核心挑战是语言符号与所指对象之间的逻辑链断裂。<br>
根据语言学中的语义三角模型，语言传递意义的过程需要符号、所指对象和概念的共同参与。符号与所指对象没有直接的逻辑联系，这种联系需要人脑中的概念作为中介来建立。<br>
但是机器缺乏人类在日常生活中形成的概念，难以直接将符号对应于对象。所以NLP的任务是在数字空间中还原概念层。</li>
<li>除此之外，语言的歧义性、上下文依赖性、世界常识依赖性以及社会文化性也给NLP带来了极大的挑战。<br>
<strong>歧义性</strong>和<strong>上下文依赖性</strong>意味着符号与意义往往具有多重对应关系，需要依据上下文语境进行推断。<br>
<strong>常识依赖性</strong>意味着语言的理解与生成需要符合物理世界的常识，即需要让机器了解我们所处世界的运行规律。<br>
<strong>社会文化性</strong>意味着自然语言中包含大量社会文化的产物，例如俚语、方言、网络用语以及比喻、象征，需要让机器了解人类社会文化的背景。</li>
<li>综上所述，自然语言处理面临的挑战来自多个领域，需要语言学、信息科学、认知科学等多个学科共同参与，具有交叉学科的性质。</li>
</ul>
<h2 id="二自然语言处理的发展历程">二、自然语言处理的发展历程</h2>
<h3 id="1语言计算的萌芽阶段1940s-1960s">1.语言计算的萌芽阶段(1940s-1960s)</h3>
<ul>
<li>自然语言语言处理的研究起始于<strong>机器翻译</strong>，该时期的技术路线以结构主义语言学和符号主义人工智能为理论基础，通过书写语法规则和词典来描述语言。</li>
<li>1954年，Georgetown-IBM实验首次公开展示了机器翻译系统，将俄语自动翻译成英语，使人们对机器翻译领域十分乐观。</li>
<li>但机器翻译很快体现出了其局限性。第一，规则无法覆盖语言无穷的可能性和例外情况；第二，严重依赖语言学专家手工编写规则，耗时耗力，难以规模化；第三，机器学习无法处理歧义问题。</li>
<li>1966年，ALPAC报告全面否定了机器翻译，呼吁转向计算语言学的基础研究。</li>
</ul>
<h3 id="2基于统计的方法兴起1980s-2010s">2.基于统计的方法兴起(1980s-2010s)</h3>
<ul>
<li>随着大规模语料库的出现，统计学习方法开始取代规则系统，成为NLP的主流范式。其背后的理论依据是语言的使用模式可以通过概率和统计模型来捕捉，不需要完全依赖人工规则。</li>
<li>该阶段的<strong>技术路线</strong>包括以下流程：<br>
<strong>数据收集与预处理</strong>：收集大规模的真实文本构建语料库，并进行情感标签、实体标签、翻译结果等基本处理，在后续训练中用于计算损失，直接驱动模型参数更新。<br>
<strong>特征工程</strong>：从文本中提取可供模型使用的关键特征，例如词性标签、句法结构等，为任务提供更好的输入表示。<br>
<strong>统计模型构建</strong>：利用概率论和信息论，建立数学模型来描述语言现象。核心思想是找到最有可能的输出。<br>
<strong>模型训练与解码</strong>：用数据训练模型，通过计算损失→计算梯度→更新参数的不断迭代，使得众多可能中概率最大的一项即为想要的输出</li>
<li><strong>关键技术与模型</strong><br>
<strong>核心思想：噪声信道模型</strong><br>
它将翻译、语音识别等问题转化为“噪声信道”过程：原始信息（正确的翻译结果）经过一个噪声信道（翻译过程）后，变成被观测到的失真信息（待处理文本）。NLP的任务就是根据观测到的扭曲信息，通过贝叶斯定理找到最有可能的原始信息。<br>
<strong>n-gram语言模型</strong><br>
语言模型的任务是预测下一个词出现的可能性，n-gram语言模型是实现这一目标的一种简单且高效的统计方法。它基于马尔科夫假设：一个词出现的概率只依赖于它前面的n-1个词。<br>
n-gram概率通过在文本语料库中计数，然后使用最大似然估计来计算。由于存储n-gram概率所需空间随n成指数级增长，为了平衡效果和存储空间，n一般取3或4。<br>
n-gram模型也存在明显的缺点：(1)数据稀疏严重；(2)无法捕捉长距离依赖；(3)无法捕捉词语的相似性。<br>
<strong>隐马尔科夫模型（HMM）</strong><sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup><br>
HMM是一种概率图模型，用于描述含有隐含未知参数的马尔科夫过程。其核心思想是根据观测到的序列，来推断最有可能产生它的<strong>隐藏状态序列</strong>。<br>
HMM由三个概率分布和两个集合完全定义<br>
(1)隐藏状态集合：在词性标注中为{名词，动词，形容词……}<br>
(2)观测状态集合：在词性标注中体现为词汇表{我，喜欢，苹果……}<br>
(3)初始状态概率分布：系统在初始时刻处于各个隐藏状态的概率<br>
(4)状态转移矩阵：描述隐藏状态(i->j)之间转换的概率<br>
(5)发射概率矩阵：描述从隐藏状态生成观测状态的概率<br>
常见应用：词性标注（词语序列->词性标签序列）、命名实体识别（词语序列->实体标签序列）、语音识别(声学信号->文本序列)</li>
</ul>
<p><strong>支持向量机（SVM）</strong><br>
SVM是一种分类算法，其核心任务是找到一种最可靠的方式，将属于不同类别的数据点区分开。<br>
SVM的目标是找到<strong>最优安全边界</strong>，即该边界到离自己最近的、来自两类类别的数据点的距离最大。这些离最优直线距离最近的数据点被称为<strong>支持向量</strong>。对于复杂的数据，SVM会运用<strong>核函数</strong><sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup>进行升维处理，在高维空间中找到最优的超平面。<br>
SVM的决策基于少量关键样本，追求最佳泛化能力，能够处理非线性问题，被广泛应用于文本分类、情感分析等任务。</p>
<ul>
<li><strong>典型应用</strong>：(1) Google PageRank 搜索引擎；(2) 基于统计机器翻译的 Google 翻译</li>
</ul>
<h3 id="3深度学习革命2013-2017">3.深度学习革命(2013-2017)</h3>
<ul>
<li>统计方法学习下，模型性能严重依赖于特征工程，而深度学习的核心能力则是自动从原始数据中学习多层次的特征表示。</li>
<li>首先，我们对<strong>深度学习</strong>和<strong>神经网络</strong>两个概念进行辨析<br>
<strong>深度学习</strong>是一个研究领域，关注如何从数据中自动学习层次化的特征表示，在2000s被广泛推广。<br>
<strong>神经网络</strong>是一种具体的模型架构和算法，关注神经元如何连接、激活函数如何作用、误差如何反向传播以更新权重等，概念起源于1940s。在深度学习中，包含了八大神经网络，在不同领域展现出卓越的性能。<br>
<strong>深度神经网络(DNN)<strong>特指具有多个隐藏层的神经网络，核心能力是通过多个隐藏层，逐级从数据中自动学习从低级到高级的特征。
<strong>神经网络</strong>是实现</strong>深度学习</strong>的主要技术路径，包括八大神经网络：<strong>卷积神经网络(CNN)</strong>（主要用于图像处理任务）、<strong>循环神经网络(RNN)</strong>（适用于时间序列和自然语言处理任务）、<strong>生成对抗网络(GAN)</strong>（用于生成与真实数据相似的新数据，例如图像生成）、<strong>图神经网络(GNN)</strong>（用于处理图结构数据）、<strong>长短期记忆网络(LSTM)</strong>（适用于需要捕捉长期依赖关系的序列任务）、<strong>Transformer</strong>（引入自注意力机制）、<strong>深度Q网络(DQN)</strong>（适用于强化学习中离散动作空间问题）、<strong>扩张残差网络(DRN)</strong>（适用于图像分割任务）。</li>
<li><strong>核心技术突破</strong><br>
在这一阶段，一系列核心技术的突破使得模型的泛化能力、长文本处理能力大大提升。<br>
<strong>词向量(Word2Vec)</strong><br>
每个词被表示为一个稠密的、低维的实数向量，向量空间中的几何关系（距离、方向）可以表示语义和语法关系，使得模型的泛化能力大大增强。<br>
<strong>循环神经网络与长短期记忆网络</strong><br>
循环神经网络（RNN）的变体长短期记忆网络（LSTM）以及门控循环单元（GRU）成为处理文本序列的主力。<br>
LSTM通过<strong>门控</strong><sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup>机制，可以选择性地记忆和遗忘信息，从而能够捕捉长距离的上下文依赖关系，使得构建强大的序列到序列模型成为可能。</li>
</ul>
<p><strong>序列到序列（Seq2Seq）模型</strong><br>
提供了一个通用的端到端<strong>编码器-解码器</strong><sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup>框架。编码器（通常是一个RNN/LSTM）将输入序列压缩成一个上下文向量；解码器（另一个RNN/LSTM）根据这个向量生成输出序列。<br>
Seq2Seq仍然具有局限性，对于长序列，其将整个输入序列信息压缩到一个固定长度的上下文向量中，会造成初始部分信息的稀释和丢失。</p>
<p><strong>注意力机制</strong><br>
注意力机制允许解码器在生成每一个词的时候，通过计算权重，动态地、自动地去关注输入中与之最相关的部分。<br>
注意力机制极大提升了长序列处理能力，为完全基于注意力的Transformer模型的诞生埋下了伏笔。</p>
<h3 id="4transformer架构时代2017-至今">4.Transformer架构时代(2017-至今)</h3>
<ul>
<li>
<p>Transformer模型完全基于自注意力机制，放弃了循环和卷积，主要由编码器和解码器两大模块构成。</p>
</li>
<li>
<p>编码器负责将输入序列表示为一个伏旱上下文信息的表示序列。其由多个完全相同的层堆叠而成。每一层包含两个子层：<strong>多头自注意力层</strong><sup><a href="#user-content-fn-5" id="user-content-fnref-5" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup>和<strong>前馈神经网络</strong><sup><a href="#user-content-fn-6" id="user-content-fnref-6" data-footnote-ref="" aria-describedby="footnote-label">6</a></sup>。同时，使用残差连接缓解梯度消失问题，并通过层归一化稳定训练过程。</p>
</li>
<li>
<p>解码器负责根据编码器的输出和已生成的输出序列，逐个生成目标序列。其同样由多个相同的层堆叠而成，每一层包含三个子层：<strong>掩码多头自注意力层</strong><sup><a href="#user-content-fn-7" id="user-content-fnref-7" data-footnote-ref="" aria-describedby="footnote-label">7</a></sup>、<strong>多头编码-解码注意力层</strong><sup><a href="#user-content-fn-8" id="user-content-fnref-8" data-footnote-ref="" aria-describedby="footnote-label">8</a></sup>和<strong>前馈神经网络</strong>。同样使用残差连接和层归一化。</p>
</li>
</ul>
<h2 id="三基于transformer的预训练大模型plm">三、基于transformer的预训练大模型(PLM)</h2>
<h3 id="1qkv范式9">1.QKV范式<sup><a href="#user-content-fn-9" id="user-content-fnref-9" data-footnote-ref="" aria-describedby="footnote-label">9</a></sup></h3>
<ul>
<li>QKV(Query,Key,Value)范式是实现Attention机制的核心机理。</li>
<li>Query代表这个词想要寻找什么，Key代表这个词能够提供什么信息，Value代表这个词的实际内容是什么。</li>
<li>QKV的创建：句子中每一个词向量都会分别与三个不同权重的矩阵相乘，分别对应生成Query、Key和Value。</li>
<li>交互计算：将某个词的Query向量和句子中所有词的Key向量进行点积，通过Softmax函数进行归一化，得到每一个词和该词的注意力权重。最后以注意力权重为系数，对所有词的Value向量加权求和，得到该词在新的上下文中的表示。</li>
<li>区分为三个向量的优势:<br>
(1)QK分离：实现了软寻址，不是简单根据精确匹配来查找，而是通过向量的相似度进行柔性匹配，增强了模型的泛化能力。<br>
(2)V的分离：实现了信息和寻址的分离，可以快速利用标签判断相关性。</li>
</ul>
<h3 id="2预训练">2.预训练</h3>
<ul>
<li><strong>预训练</strong>是指让模型在海量无标注文本数据上，进行无监督/弱监督训练，学习通用的语言表示，完成通用或特定的自然语言处理任务。预训练通常占据大模型训练时间的90%以上。</li>
<li>大模型经过预训练之后，再经过<strong>监督微调(SFT)</strong>（学习回答格式）、<strong>奖励模型训练(RM)</strong>（学会评判标准）、<strong>强化学习优化(RLHF)</strong><sup><a href="#user-content-fn-10" id="user-content-fnref-10" data-footnote-ref="" aria-describedby="footnote-label">10</a></sup>（自我优化）的流程，就能成为成熟的大模型。</li>
</ul>
<h3 id="3两大预训练流派">3.两大预训练流派</h3>
<ul>
<li>基于Transformer，形成了两大预训练流派，分别面向<strong>语言理解</strong>和<strong>语言生成</strong>。</li>
<li><strong>BERT流派：面向语言理解</strong><br>
BERT流派的核心架构为双向Transformer编码器，同时利用遮盖词左右两侧的上下文预测被遮盖词是什么，在语言理解方面具有天然优势。</li>
<li><strong>GPT流派：面向语言生成</strong><br>
GPT流派的核心架构为自回归Transformer解码器，利用当前词之前的词预测下一个词，擅长完成语言生成领域的任务。</li>
</ul>
<h3 id="4关于预训练大模型的经验定律">4.关于预训练大模型的经验定律</h3>
<ul>
<li><strong>规模定律(scaling law)</strong><br>
模型性能依赖于模型参数量、训练数据量和计算量。其他条件不变，随着单一指标的幂次增长，模型性能线性增长。<br>
规模定律意味着只要通过简单地扩大模型，增加数据和算力，就能够可靠地提升性能，为AI领域的“军备竞赛”提供了理论依据。</li>
<li><strong>涌现能力</strong><br>
当模型规模超过某个临界阈值时，模型会突然表现出在较小规模模型中不存在的能力。<br>
它表明模型根据scaling law量变到一定程度，会产生质的变化。</li>
<li><strong>推断时规模定律(test-time scaling law)</strong>
模型的推理能力可以通过在生成每个答案时投入更多计算量而得到显著提升。<br>
基于该定律，提出了<strong>快思考</strong>和<strong>慢思考</strong>的概念。<strong>快思考</strong>指模型凭直觉快速生成提个答案；<strong>慢思考</strong>指模型生成n个候选答案，并使用<strong>验证器</strong>(verifier)从中挑选出最优的k个，选择最优解进行下一步，从而提升推理能力。</li>
<li><strong>顿悟(aha moment)</strong>
模型在训练过程中，对于某些技能或知识的掌握可能在某一个训练节点突然发生性能陡增。这往往会在输出风格上体现为拟人化的自我修正，例如“不对，让我重新思考一下……”。</li>
</ul>
<h2 id="四deepseek的创新与突破">四.DeepSeek的创新与突破</h2>
<ul>
<li>2025年DeepSeek发布，使用相对较少的训练数据达到了与更大规模数据训练的模型相近的性能，体现了其技术架构和训练方法层面的优势。</li>
</ul>
<h3 id="1混合专家系统moe架构">1.混合专家系统(MoE)架构</h3>
<ul>
<li>MoE架构的大模型由大量擅长处理特定领域任务的“专家”构成，在处理具体输入时只有一小部分专家被激活，和传统密集模型相比有效节省了计算量。</li>
<li>DeepSeek V3使用了257个专家网络，每次仅激活8个，降低了训练和推理成本。<sup><a href="#user-content-fn-11" id="user-content-fnref-11" data-footnote-ref="" aria-describedby="footnote-label">11</a></sup></li>
</ul>
<h3 id="2多头隐式注意力mla">2.多头隐式注意力(MLA)</h3>
<ul>
<li>标准注意力机制在生成每一个token时，都需要将当前新生成的词与之前所有已生成的词进行注意力计算，这意味着随着生成长度的增加，需要存储和处理的KV矩阵会线性增长，影响了模型处理长上下文的能力。</li>
<li>MLA的核心思想则是将庞大的KV矩阵压缩到一个隐式状态（一个固定大小的张量）中，有效提高内存效率和推理速度。</li>
<li>工作原理：MLA通过递归不断根据新输入更新隐式状态，在需要计算注意力时，通过一个轻量的前馈网络，从隐式状态中解码出当前步骤所需的Key和Value。</li>
</ul>
<h3 id="3dualpipe流水线并行技术">3.DualPipe流水线并行技术</h3>
<ul>
<li>通过优化GPU之间的工作负载分配，减少GPU的空闲等待时间，直接提升了算力利用率，是当前算力稀缺背景下的一项核心能力。</li>
</ul>
<h3 id="4群体相对策略优化grpo">4.群体相对策略优化(GRPO)</h3>
<ul>
<li>这是一种更加直接、稳定的强化学习算法，不依赖奖励模型，直接从人类反馈数据中学习，减少了训练环节，降低了系统复杂性，并可能避免因奖励模型偏差带来的问题。</li>
</ul>
<h3 id="5顿悟机制利用aha-moment">5.顿悟机制利用(aha moment)</h3>
<ul>
<li>DeepSeek在生成答案遇到错误时，具备倒退步骤进行自我纠错的自我修正能力，体现出了对人类反思和验证过程的模拟。</li>
</ul>
<h2 id="五nlp研究瓶颈与发展方向">五、NLP研究瓶颈与发展方向</h2>
<h3 id="1研究瓶颈">1.研究瓶颈</h3>
<ul>
<li><strong>幻觉问题</strong><br>
模型学习的是统计规律而非事实真理，可能会输出违背事实的信息，使之在医疗、法律等高风险领域的可靠应用受到挑战。</li>
<li><strong>黑箱困境</strong><br>
深度神经网络巨大的参数量和非线性变换，使得其决策过程对人类而言不透明，这导致了调试困难、信任缺失、责任边界模糊等问题。</li>
<li><strong>推理能力薄弱</strong><br>
模型在需要多步推理、数学计算或解答物理问题的任务上表现不佳，可能只是记忆了推理模式，而非掌握了推理能力，缺乏对物理世界和社会的基本认知。</li>
</ul>
<h3 id="2发展方向">2.发展方向</h3>
<ul>
<li><strong>检索增强生成</strong><br>
在模型生成答案前，先从外部的、可控的知识源(如数据库、网页)中检索相关信息，并以此为依据进行生成，提升事实准确性。</li>
<li><strong>可解释AI和神经符号AI</strong><br>
<strong>可解释AI</strong>是指智能体以一种可解释的方式与人类用户互动，从而取得人类信任，满足监管要求。可以通过发展简单、透明的模型和对复杂黑箱模型创建事后解释等途径推动AI可解释化。<sup><a href="#user-content-fn-12" id="user-content-fnref-12" data-footnote-ref="" aria-describedby="footnote-label">12</a></sup></li>
</ul>
<p><strong>神经符号AI</strong>指将神经网络的感知能力和符号逻辑的推理能力相结合，提高应对复杂情境的能力，保证推理过程透明可靠。</p>
<ul>
<li><strong>AI反馈强化学习(RLAIF)</strong><br>
使用AI模型来提供反馈，让模型之间进行自我博弈与改进，实现自我进化，从而大幅降低对齐成本，并可能产生超越人类水平的策略。</li>
<li><strong>具身智能体和多智能体系统</strong><br>
<strong>具身智能体</strong>通过和环境互动，获得直接、具身的经验，从而学习语言无法完全描述的物理常识。<br>
<strong>多智能体系统</strong>让多个由大模型驱动的智能体在复杂环境中进行协作、竞争与交流，从而催生出社会智能和复杂的任务规划能力，具备并行探索和关注点分离等单智能体不具备的能力，是解决开放世界问题的关键。<sup><a href="#user-content-fn-13" id="user-content-fnref-13" data-footnote-ref="" aria-describedby="footnote-label">13</a></sup></li>
</ul>
<h2 id="ai协作记录">AI协作记录</h2>
<div style="font-size: small;">
<p><strong>Prompt 1:</strong>
关于LSTM、Seq2Seq模型和Attention机制。
LSTM、Seq2Seq模型和Attention机制都是针对提升长序列处理能力所进行的技术创新，这三者之间是否是一种递进关系？</p>
<p><strong>Answer:</strong>
<strong>DeepSeek</strong>：好的，我们来更详细地阐述这三者之间的递进关系。这种关系可以被清晰地描述为 <strong>“发现问题 -> 创新架构 -> 革命机制”</strong> 的三步演进。</p>
<p><strong>1. LSTM：解决“记忆丢失”问题的单元级突破</strong></p>
<ul>
<li><strong>前身问题</strong>：传统的RNN在处理长序列时，信息需要在时间步间多次传递，导致<strong>梯度消失/爆炸</strong>，模型难以学到长距离的依赖关系，即存在“短期记忆”问题。</li>
<li><strong>核心创新</strong>：LSTM引入了<strong>门控机制</strong>，包括输入门、遗忘门和输出门。这套机制允许细胞状态有选择地保留或丢弃信息，从而在序列中维持一条相对稳定的“信息高速公路”。</li>
<li><strong>作用</strong>：LSTM作为<strong>一个更强大的神经元</strong>，显著增强了循环网络保留长期上下文信息的能力，是处理长序列的<strong>必要基础</strong>。但模型的整体输入输出架构并未改变。</li>
</ul>
<p><strong>2. Seq2Seq：解决“结构映射”问题的框架级创新</strong></p>
<ul>
<li><strong>前身问题</strong>：传统的固定输入-输出模型无法处理像机器翻译这类<strong>源语言和目标语言序列长度可变且不对等</strong>的任务。</li>
<li><strong>核心创新</strong>：提出了<strong>编码器-解码器</strong>架构。
<ul>
<li><strong>编码器</strong>（通常是一个RNN/LSTM）：将整个源序列编码成一个<strong>固定长度的上下文向量</strong>。</li>
<li><strong>解码器</strong>（另一个RNN/LSTM）：基于这个“思维向量”逐步生成目标序列。</li>
</ul>
</li>
<li><strong>新瓶颈</strong>：这个固定长度的上下文向量成为了致命的<strong>信息瓶颈</strong>。对于长序列，源端的所有信息必须被压缩进这个固定维度的向量中，导致大量细节丢失，模型性能随序列增长而急剧下降。</li>
</ul>
<p><strong>3. Attention机制：解决“信息瓶颈”问题的范式级革命</strong></p>
<ul>
<li><strong>针对问题</strong>：直接且优雅地解决了Seq2Seq模型的<strong>信息瓶颈</strong>问题。</li>
<li><strong>核心创新</strong>：它摒弃了将整个序列压缩为一个向量的做法。在解码器生成<strong>每一个词</strong>时，Attention机制允许它<strong>直接访问编码器对所有输入词的完整状态</strong>，并通过计算一组可学习的<strong>注意力权重</strong>，来决定当前时刻应该重点关注输入序列的哪些部分。
<ul>
<li>例如，在翻译“I love China”为“我爱中国”时，生成“爱”时模型会高度关注“love”，生成“中国”时则会重点关注“China”。</li>
</ul>
</li>
<li><strong>革命性影响</strong>：
<ol>
<li><strong>性能飞跃</strong>：模型不再受限于固定长度向量，处理长序列的能力获得质的提升。</li>
<li><strong>可解释性</strong>：注意力权重可视化后，能直观展示模型的“对齐”过程。</li>
<li><strong>催生Transformer</strong>：Attention的成功证明其本身足以建模序列依赖，从而催生了完全基于自注意力、抛弃了循环结构的<strong>Transformer</strong>模型，实现了对RNN/LSTM的范式替代。</li>
</ol>
</li>
</ul>
<p><strong>演进总结</strong></p>
<p>这三者的关系是层层递进的：</p>
<ul>
<li><strong>LSTM</strong> 让RNN有了处理长序列的 <strong>“体力”</strong>（长期记忆）。</li>
<li><strong>Seq2Seq</strong> 为处理序列到序列任务设计了 <strong>“舞台”</strong>（编码器-解码器框架），但舞台有个<strong>天花板太低</strong>（信息瓶颈）的缺陷。</li>
<li><strong>Attention机制</strong> <strong>直接拆掉了天花板</strong>（动态全局访问），不仅完美解决了舞台的缺陷，其强大的能力还催生了全新的、更高效的 <strong>“剧院”</strong>（Transformer）。</li>
</ul>
<p>因此，这是一个典型的<strong>技术迭代案例</strong>：后一项技术是为了解决前一项技术所暴露出的核心弱点而诞生，并最终引领了整个领域的发展方向。</p>
<p><strong>Prompt 2:</strong>
噪声信号通道模型和端到端学习。
噪声信道模型和端到端学习有什么区别。</p>
<p><strong>Answer:</strong>
<strong>DeepSeek</strong>：<strong>噪声信道模型</strong>与<strong>端到端学习</strong>的根本区别在于<strong>问题解决范式</strong>的不同。</p>
<p><strong>噪声信道模型</strong>采用<strong>分治策略</strong>，将任务（如机器翻译）拆解为独立的子模块（如翻译模型和语言模型），各模块分别训练优化，最后通过解码器组合。它基于贝叶斯定理，从生成式视角逆向推断源信息，需要人工设计流水线，依赖不同类型的数据训练不同模块。</p>
<p><strong>端到端学习</strong>采用<strong>统一映射策略</strong>，用一个单一的神经网络（如Transformer）直接学习从输入到输出的完整映射。模型内部通过注意力机制等动态学习所有中间特征和步骤，所有参数为同一目标联合优化，只需端到端的对齐数据。</p>
<p>简言之，<strong>前者是“先分解，后集成”的模块化工程思想，后者是“一口入，一口出”的数据驱动黑箱学习</strong>。端到端方法因其更强的表征能力和更简单的训练流程，已成为当前主流，但它也牺牲了噪声信道模型固有的模块清晰性和可解释性。</p>
<p><strong>Prompt 3:</strong>
能否将两者结合起来实现AI的可解释性。</p>
<p><strong>Answer:</strong>
<strong>DeepSeek</strong>：<strong>完全可以结合，这是实现可解释AI的关键路径。</strong></p>
<p>核心方法是 <strong>“内嵌符号推理”</strong> 和 <strong>“外挂知识检索”</strong>：</p>
<ol>
<li>
<p><strong>神经符号AI</strong>：让<strong>神经网络</strong>（端到端部分）处理感知和模式识别，提取高级概念；让<strong>符号系统</strong>（噪声信道思想）对这些概念进行逻辑推理和决策。解释自然产生于符号推理链中。</p>
</li>
<li>
<p><strong>检索增强生成</strong>：在生成前，先从一个<strong>外部可信知识库</strong>（相当于清晰的信道源）检索依据。模型基于这些证据生成答案，并可展示证据来源，直接解释“答案从何而来”。</p>
</li>
</ol>
<p>简言之，就是用端到端模型获得<strong>强大能力</strong>，用模块化、符号化的思想构建<strong>解释框架</strong>。两者结合是实现<strong>可信AI</strong>的务实方向。</p>
</div>
<section data-footnotes="" class="footnotes"><h2 class="sr-only" id="footnote-label">Footnotes</h2>
<ol>
<li id="user-content-fn-1">
<p><a href="https://zhuanlan.zhihu.com/p/88362664">https://zhuanlan.zhihu.com/p/88362664</a> <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-2">
<p><a href="https://zhuanlan.zhihu.com/p/11314874727">https://zhuanlan.zhihu.com/p/11314874727</a> <a href="#user-content-fnref-2" data-footnote-backref="" aria-label="Back to reference 2" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-3">
<p><a href="https://blog.csdn.net/liruiqiang05/article/details/147124187#">https://blog.csdn.net/liruiqiang05/article/details/147124187#</a> <a href="#user-content-fnref-3" data-footnote-backref="" aria-label="Back to reference 3" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-4">
<p><a href="https://c.biancheng.net/view/4tfk8re.html">https://c.biancheng.net/view/4tfk8re.html</a> <a href="#user-content-fnref-4" data-footnote-backref="" aria-label="Back to reference 4" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-5">
<p>多头自注意力层从多个角度分析每个词和其他所有词的关系，生成包含全局上下文的向量。 <a href="#user-content-fnref-5" data-footnote-backref="" aria-label="Back to reference 5" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-6">
<p>前馈神经网络根据自注意力层的输出向量进行独立变换 <a href="#user-content-fnref-6" data-footnote-backref="" aria-label="Back to reference 6" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-7">
<p>掩码多头自注意力层关注已生成的输出序列，确保在生成时不被后续词影响。 <a href="#user-content-fnref-7" data-footnote-backref="" aria-label="Back to reference 7" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-8">
<p>多头编码-解码注意力层是连接编码器和解码器的桥梁，同时关注解码器上一层的输出和编码器的最终输出，使得在生成每一个词时能选择性关注输入序列中最相关部分。 <a href="#user-content-fnref-8" data-footnote-backref="" aria-label="Back to reference 8" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-9">
<p><a href="https://www.zhihu.com/tardis/zm/art/414084879?source_id=1005">https://www.zhihu.com/tardis/zm/art/414084879?source_id=1005</a> <a href="#user-content-fnref-9" data-footnote-backref="" aria-label="Back to reference 9" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-10">
<p>目前最核心的三大RL算法是PPO、DPO和GRPO。 <a href="#user-content-fnref-10" data-footnote-backref="" aria-label="Back to reference 10" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-11">
<p>数据来自老师课堂ppt。 <a href="#user-content-fnref-11" data-footnote-backref="" aria-label="Back to reference 11" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-12">
<p><a href="https://zhuanlan.zhihu.com/p/610069413">https://zhuanlan.zhihu.com/p/610069413</a> <a href="#user-content-fnref-12" data-footnote-backref="" aria-label="Back to reference 12" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-13">
<p><a href="https://zhuanlan.zhihu.com/p/1928636720796136414">https://zhuanlan.zhihu.com/p/1928636720796136414</a> <a href="#user-content-fnref-13" data-footnote-backref="" aria-label="Back to reference 13" class="data-footnote-backref">↩</a></p>
</li>
</ol>
</section> </div> </article>   </main> <footer class="border-t border-slate-200 mt-20 py-12 text-center text-slate-500 text-sm"> <p>
&copy; 2026 OODS. All rights reserved.
</p> </footer> </body></html>