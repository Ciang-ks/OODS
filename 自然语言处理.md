# 自然语言处理 赵东岩  
## 一、自然语言处理的基本概念  
### 1.广义语言与自然语言  
- **广义语言**是指一套共同采用的沟通符号、表达方式与处理原则，包括自然语言、动物信号、计算机语言等，具有多模态特征。  
- **自然语言**是指随人类文化演化的自然形成的语言，不包括计算机语言和形式语言，具有高歧义、高上下文依赖的特征。  
### 2.自然语言处理(NLP)  
- NLP是指对人类特有的书面形式和口头形式的自然语言的信息进行各种类型处理和加工。  
- NLP的核心任务包括自然语言理解(NLU)和自然语言生成(NLG)。  
- NLU旨在从自然语言文本或语音中提取出确切的含义。典型的NLU任务包括词性标注、句法分析、语义角色标注、情感分析等。  
- NLG旨在让机器写出通顺、合理、符合语境的人类语言。典型NLG任务包括机器翻译、文本摘要、对话系统等  
- NLP一般采用多层处理结构，包括词法、句法、语义、语用四个层面
### 3.自然语言处理的挑战  
- NLP面临的核心挑战是语言符号与所指对象之间的逻辑链断裂。  
根据语言学中的语义三角模型，语言传递意义的过程需要符号、所指对象和概念的共同参与。符号与所指对象没有直接的逻辑联系，这种联系需要人脑中的概念作为中介来建立。  
但是机器缺乏人类在日常生活中形成的概念，难以直接将符号对应于对象。所以NLP的任务是在数字空间中还原概念层。
- 除此之外，语言的歧义性、上下文依赖性、世界常识依赖性以及社会文化性也给NLP带来了极大的挑战。  
**歧义性**和**上下文依赖性**意味着符号与意义往往具有多重对应关系，需要依据上下文语境进行推断。  
**常识依赖性**意味着语言的理解与生成需要符合物理世界的常识，即需要让机器了解我们所处世界的运行规律。  
**社会文化性**意味着自然语言中包含大量社会文化的产物，例如俚语、方言、网络用语以及比喻、象征，需要让机器了解人类社会文化的背景。  
- 综上所述，自然语言处理面临的挑战来自多个领域，需要语言学、信息科学、认知科学等多个学科共同参与，具有交叉学科的性质。

## 二、自然语言处理的发展历程
### 1.语言计算的萌芽阶段(1940s-1960s)
- 自然语言语言处理的研究起始于**机器翻译**，该时期的技术路线以结构主义语言学和符号主义人工智能为理论基础，通过书写语法规则和词典来描述语言。
- 1954年，Georgetown-IBM实验首次公开展示了机器翻译系统，将俄语自动翻译成英语，使人们对机器翻译领域十分乐观。
- 但机器翻译很快体现出了其局限性。第一，规则无法覆盖语言无穷的可能性和例外情况；第二，严重依赖语言学专家手工编写规则，耗时耗力，难以规模化；第三，机器学习无法处理歧义问题。
- 1966年，ALPAC报告全面否定了机器翻译，呼吁转向计算语言学的基础研究。
### 2.基于统计的方法兴起(1980s-2010s)
- 随着大规模语料库的出现，统计学习方法开始取代规则系统，成为NLP的主流范式。其背后的理论依据是语言的使用模式可以通过概率和统计模型来捕捉，不需要完全依赖人工规则。
- 该阶段的**技术路线**包括以下流程：  
**数据收集与预处理**：收集大规模的真实文本构建语料库，并进行情感标签、实体标签、翻译结果等基本处理，在后续训练中用于计算损失，直接驱动模型参数更新。  
**特征工程**：从文本中提取可供模型使用的关键特征，例如词性标签、句法结构等，为任务提供更好的输入表示。  
**统计模型构建**：利用概率论和信息论，建立数学模型来描述语言现象。核心思想是找到最有可能的输出。  
**模型训练与解码**：用数据训练模型，通过计算损失→计算梯度→更新参数的不断迭代，使得众多可能中概率最大的一项即为想要的输出
- **关键技术与模型**  
**核心思想：噪声信道模型**  
它将翻译、语音识别等问题转化为“噪声信道”过程：原始信息（正确的翻译结果）经过一个噪声信道（翻译过程）后，变成被观测到的失真信息（待处理文本）。NLP的任务就是根据观测到的扭曲信息，通过贝叶斯定理找到最有可能的原始信息。  
**n-gram语言模型**  
语言模型的任务是预测下一个词出现的可能性，n-gram语言模型是实现这一目标的一种简单且高效的统计方法。它基于马尔科夫假设：一个词出现的概率只依赖于它前面的n-1个  词。  
n-gram概率通过在文本语料库中计数，然后使用最大似然估计来计算。由于存储n-gram概率所需空间随n成指数级增长，为了平衡效果和存储空间，n一般取3或4。  
n-gram模型也存在明显的缺点：(1)数据稀疏严重；(2)无法捕捉长距离依赖；(3)无法捕捉词语的相似性。  
**隐马尔科夫模型（HMM）**
HMM是一种概率图模型，用于描述含有隐含未知参数的马尔科夫过程。其核心思想是根据观测到的序列，来推断最有可能产生它的**隐藏状态序列**。  
HMM由三个概率分布和两个集合完全定义  
(1)隐藏状态集合：在词性标注中为{名词，动词，形容词……}  
(2)观测状态集合：在词性标注中体现为词汇表{我，喜欢，苹果……}  
(3)初始状态概率分布：系统在初始时刻处于各个隐藏状态的概率  
(4)状态转移矩阵：描述隐藏状态(i->j)之间转换的概率  
(5)发射概率矩阵：描述从隐藏状态生成观测状态的概率  
常见应用：词性标注（词语序列->词性标签序列）、命名实体识别（词语序列->实体标签序列）、语音识别(声学信号->文本序列)  
**支持向量机（SVM）**  
SVM是一种分类算法，其核心任务是找到一种最可靠的方式，将属于不同类别的数据点区分开。  
SVM的目标是找到**最优安全边界**，即该边界到离自己最近的、来自两类类别的数据点的距离最大。这些离最优直线距离最近的数据点被称为**支持向量**。对于复杂的数据，SVM会运用**核函数**进行升维处理，在高维空间中找到最优的超平面。  
SVM的决策基于少量关键样本，追求最佳泛化能力，能够处理非线性问题，被广泛应用于文本分类、情感分析等任务。  
- **典型应用**：(1) Google PageRank 搜索引擎；(2) 基于统计机器翻译的 Google 翻译
### 3.深度学习革命(2013-2017)
- 统计方法学习下，模型性能严重依赖于特征工程，而深度学习的核心能力则是自动从原始数据中学习多层次的特征表示。  
- **概念辨析**  
**深度学习**是一个研究领域，关注如何从数据中自动学习层次化的特征表示，在2000s被广泛推广。  
**神经网络**是一种具体的模型架构和算法，关注神经元如何连接、激活函数如何作用、误差如何反向传播以更新权重等，概念起源于1940s。包括八大神经网络    
**深度神经网络(DNN)**特指具有多个隐藏层的神经网络，核心能力是通过多个隐藏层，逐级从数据中自动学习从低级到高级的特征。
**神经网络**是实现**深度学习**的主要技术路径，包括八大神经网络：**卷积神经网络(CNN)**（主要用于图像处理任务）、**循环神经网络(RNN)**（适用于时间序列和自然语言处理任务）、**生成对抗网络(GAN)**（用于生成与真实数据相似的新数据，例如图像生成）、**图神经网络(GNN)**（用于处理图结构数据）、**长短期记忆网络(LSTM)**（适用于需要捕捉长期依赖关系的序列任务）、**Transformer**（引入自注意力机制）、**深度Q网络(DQN)**（适用于强化学习中离散动作空间问题）、**扩张残差网络(DRN)**（适用于图像分割任务）。  
- **核心技术突破**  
**词向量(Word2Vec)**  
每个词被表示为一个稠密的、低维的实数向量，向量空间中的几何关系（距离、方向）可以表示语义和语法关系，使得模型的泛化能力大大增强。  
**循环神经网络与长短期记忆网络**  
循环神经网络（RNN）的变体长短期记忆网络（LSTM）以及门控循环单元（GRU）成为处理文本序列的主力。  
LSTM通过**门控**机制，可以选择性地记忆和遗忘信息，从而能够捕捉长距离的上下文依赖关系，使得构建强大的序列到序列模型成为可能。  
**序列到序列（Seq2Seq）模型**  
提供了一个通用的端到端**编码器-解码器**框架。编码器（通常是一个RNN/LSTM）将输入序列压缩成一个上下文向量；解码器（另一个RNN/LSTM）根据这个向量生成输出序列。  
Seq2Seq仍然具有局限性，对于长序列，其将整个输入序列信息压缩到一个固定长度的上下文向量中，会造成初始部分信息的稀释和丢失。  
**注意力机制**  
注意力机制允许解码器在生成每一个词的时候，通过计算权重，动态地、自动地去关注输入中与之最相关的部分。  
注意力机制极大提升了长序列处理能力，为完全基于注意力的Transformer模型的诞生埋下了伏笔。
### 4.Transformer架构时代(2017-至今)
- Transformer模型完全基于自注意力机制，放弃了循环和卷积，主要由编码器和解码器两大模块构成。
- 编码器负责将输入序列表示为一个伏旱上下文信息的表示序列。其由多个完全相同的层堆叠而成。每一层包含两个子层：**多头自注意力层**[^1]和**前馈神经网络**[^2]。同时，使用残差连接缓解梯度消失问题，并通过层归一化稳定训练过程。  
[^1]:多头自注意力层从多个角度分析每个词和其他所有词的关系，生成包含全局上下文的向量。  
[^2]:前馈神经网络根据自注意力层的输出向量进行独立变换  
- 解码器负责根据编码器的输出和已生成的输出序列，逐个生成目标序列。其同样由多个相同的层堆叠而成，每一层包含三个子层：**掩码多头自注意力层**[^3]、**多头编码-解码注意力层**[^4]和**前馈神经网络**。同样使用残差连接和层归一化。  
[^3]:掩码多头自注意力层关注已生成的输出序列，确保在生成时不被后续词影响。  
[^4]:多头编码-解码注意力层是连接编码器和解码器的桥梁，同时关注解码器上一层的输出和编码器的最终输出，使得在生成每一个词时能选择性关注输入序列中最相关部分。  

## 三、基于transformer的预训练大模型  
### 1.两大预训练流派  
- 基于Transformer，形成了两大预训练流派，分别面向**语言理解**和**语言生成**。  
- **BERT流派：面向语言理解**  
BERT流派的核心架构为双向Transformer编码器，同时利用遮盖词左右两侧的上下文预测被遮盖词是什么，在语言理解方面具有天然优势。  
- **GPT流派：面向语言生成**  
GPT流派的核心架构为自回归Transformer解码器，利用当前词之前的词预测下一个词，擅长完成语言生成领域的任务。  