---
title: "浅谈人工智能与数学"
type: report
author: "陈双本"
id: "2500017809"
---

## 1. 人工智能发展简史

### 1.1 起源与萌芽 (-1956)

- **神话先驱**：早在BC300，《列子·汤问》中记载的偃师造人，就展现了人类对“不可分辨真假”的仿生智能最早的想象。
- **理论奠基**：
	- **控制论与神经网络**：20世纪40年代，诺伯特·维纳提出控制论，视大脑为神经元组成的电子网络，确立了AI的物理与生物学基础。
    - **图灵测试**：1950年，阿兰·图灵将“机器能否思考”这一哲学命题转化成“行为是否可分辨”的数学/计算机科学命题。[^1]
- **正式诞生**：1956年的**达特茅斯会议**上，科学家正式提出“Artificial Intelligence”概念，标志该学科的正式诞生。

### 1.2 黄金年代 (1956-1974)

达特茅斯会议后，AI迎来了长达20年的“黄金年代”，并迅速分化为两大核心流派：
- **符号主义 (Symbolism)**：主张智能源于对符号的逻辑运算（Top-down）。在当时占据统治地位，坚信“20年内机器将以此方式超越人类”。
- **连接主义 (Connectionism)**：主张智能涌现于神经元之间的连接（Bottom-up）。标志是1958年Frank Rosenblatt 发明了**感知机 (Perceptron)**，成功实现了简单的图像识别，掀起了第一次神经网络热潮。

### 1.3 寒冬与回暖 (1974-1993)

- **第一次寒冬 (1974-1980)**：    
    Minsky与Papert在1969年从数学上证明了**XOR问题**（单层感知机无法解决异或/非线性分类问题），直接导致学界对神经网络丧失信心。[^2]
- **短暂回暖 (1980-1987)**：
	20世纪80年代，**专家系统**兴起。它试图通过硬编码人类逻辑来模拟智能，但因缺乏学习能力和可拓展性，最终引发了**第二次寒冬 (1987-1993)**。

### 1.4 数学的正名 (1990s)

- **通用函数逼近定理[^3]**： 1989年，Cybenko针对多层前馈网络（引入隐藏层和非线性激活函数），证明了其本质上可以逼近任意连续函数（类似高维泰勒展开）而非只是一维线性函数，确立了其理论上的全能性。
- **打破维数灾难**：1993年，Barron证明了神经网络逼近高维函数的误差主要取决于神经元数量，而与输入维度 d 的指数增长无关。这是AI能够处理图像、自然语言等超高维数据的数学根基。[^4]

### 1.5 大爆发时代 (2012-)

- **转折点**：2012年 **AlexNet** 在ImageNet夺冠，深度学习全面爆发。
- **里程碑技术**：
    - **ResNet (2015)**：通过残差连接[^5] - 提出了残差结构解决深度网络训练难题。（引入“跳跃连接”让信号跨层传递）解决深层梯度消失，识别率超越人类。
    - **AlphaGo**：结合强化学习与HJB方程[^6]（最优控制理论中的核心偏微分方程），攻克围棋。
    - **生成式AI**：Diffusion Model (2020) 利用随机微分方程（引入随机噪声项）生成图像；ChatGPT (2022) 展现通用语言能力。
    - **科学智能**：AlphaFold (2020) 预测蛋白质结构；DeepMind (2025) 获IMO数学金牌。

---
## 2. 人工智能前沿技术体系

### 2.1 机器学习的范式转变

- **定义**：计算机通过数据自动改进算法 Y=f(X)，本质是从无穷维到无穷维的映射。
- **趋势**：从 **Model-Centric** (精调模型结构) 转向 **Data-Centric** (专注于高质量数据的获取与清洗)。
- **三大支柱**：有监督学习 (标签映射)、无监督学习 (结构发现)、强化学习 (序列决策)。

### 2.2 基础大模型 (Foundation Models)

- **统一架构**：**Transformer** 及其核心的 **注意力机制 (Attention)** ，使得处理文本、图像、代码的架构得以统一。[^7]
- **训练逻辑**：看似简单的 **Next Token Prediction** (序列预测)，在海量参数下涌现出了逻辑推理能力。

### 2.3 局限与挑战

- **不可解释性**（Black Box）：深度学习缺乏完备的数学理论支撑。
- **可靠性问题**：逻辑推理过程不严谨，存在幻觉，甚至违背物理规律。
- **硬件瓶颈**：GPU算力与深度学习框架的“卡脖子”风险。

---
## 3. 人工智能的影响与变革

### 3.1 替代还是协助？

- **定位**：AI是**拥有无穷算力的协作者 (Co-pilot)**。
- **交互常态化**：从日常闲聊到学术研讨，与AI的交流将成为常态。
    - **效率**：与大模型的高频交互，其知识获取效率可能高于与一般同行的低效交流。
    - **技能**：**提示词工程 (Prompt Engineering)** 将成为关键软技能（如何精准提问）。
### 3.2 教育领域变革

- **课堂引入**：
    - **AI助教**：北大**Brainiac Buddy (BB)**[^8]，提供24/7的个性化辅导与答疑。
    - **北大“问学”**：探索“大班授课 + AI辅助 + 小班研讨”的新模式。
- **大学生应对策略**：
    - **祛魅与利用**：不迷信AI，但能够使用AI作为工具（代码辅助、文献梳理）。
    - **批判性思维**：针对AI的“幻觉”问题，培养核查事实与逻辑纠错的能力。

---
## 4. 展望未来：AI for Math (AI赋能数学)

### 4.1 数学发展的“卡脖子”困境

- **体系复杂**：现代数学分支极度深奥且庞杂（如代数几何与组合数学），人类数学家难以跨领域掌握全部前沿。
- **前沿难题**：复杂证明的验证耗时耗力，超出个体认知极限。

### 4.2 AI的解决方案

- **知识融合**：利用AI的海量记忆与检索，打破学科壁垒，有机融合全领域数学知识。
- **辅助研究**：
    - **自然语言表达**：降低数学门槛，通过对话辅助数学家理解复杂概念。
    - **证实猜想**：生成猜想并进行形式化证明[^9]（将自然语言证明转化为计算机可验证的代码，进行严格的逻辑推导）。
- **基础设施建设**：
    - **目标**：建立高质量、标准化的数学类数据库。
    - **产物**：打造数学领域“基础软件”，赋能数学学科发展。


## *Appendices*
---
### 课程感悟

董彬老师的课程不仅为我们梳理了人工智能的发展脉络，更提出了一个核心矛盾：**高维数学问题的求解难度与人类有限认知能力之间的矛盾**。这恰恰是当下 AI for Science (AI4S) 发展的支点所在。
本部分将尝试给出一些对AI for Math（AI for Science）方面的感想，并基于课程内容提供有关领域的一些拓展信息。

#### 1. 突破维数灾难：AI 在科学计算中的本质优势

课程中提到的 Barron 关于“无维数灾难”的证明，实际上是 AI4S 能够成立的基石。在传统科学计算中（如流体力学、量子化学），我们往往依赖网格法。但处理某些高维复杂方程时，这种传统方法的计算量会呈指数级爆炸。

AI 为这一问题提供了全新的思路：**神经算子 (Neural Operators)**。 不同于传统数值方法去求解离散点，深度学习模型会直接学习函数空间之间的映射。而神经网络对高维变量“不敏感”，这使得 AI 能够以比传统方法快 1000 倍甚至 10 万倍的速度求解复杂的物理方程。这种能力让过去无法计算的复杂系统（如全尺度的天气预报 GraphCast，或超大分子的动力学模拟）出现可解的可能。

#### 2. “黑盒”与“白盒”的博弈：物理信息神经网络

教授还提到了 AI 容易“违背物理规律”和“缺乏严谨性”。这也是目前AI+Science最激烈的战场——**数据驱动 (Data-Driven) 与 机理驱动 (Physics-Based)**。

传统的深度学习是基于数据的概率拟合（黑盒），它可能生成一张极其逼真的流体图片，但该流体可能完全违背真实物理定律。为了解决这个问题，前沿研究提出**物理信息神经网络 (PINNs)**。 PINNs 的核心思想是将物理方程（如偏微分方程 PDE）作为损失函数的一部分加入到神经网络的训练中。这样，模型不仅是在拟合数据，更是在“受物理定律约束”的解空间中寻找答案。这种**“灰盒”**模式，既利用了神经网络强大的拟合能力，又通过数学方程保证了结果的物理可信度。这完美呼应了课程中关于“数学严谨性”与“深度学习”需要结合的观点。

#### 3. AI for Math：从加速器到协作者

长久以来，计算机只是一个能够快速做数值计算的加速器。但现在，随着大模型逻辑推理能力的提升，AI 有机会成为数学前沿科研的协作者。

除了课程中提到的自动定理证明，DeepMind 的 AlphaGeometry 和 AIMO 的进展表明，AI 正在开始具备“直觉”。在解决几何题时，AI 不再是暴力搜索，而是像人类一样学会了“添加辅助线”。这种“直觉”的涌现，或许意味着 AI 正在触及数学创造力的边缘。

#### 4. 结语

我们正处于科学发现范式转移的风暴眼：第一范式是实验科学（记录现象），第二范式是理论科学（牛顿定律），第三范式是计算科学（模拟），而现在我们正进入全新范式——**数据密集型科学发现**。
未来的科研，可能不再是单纯地推导公式或做实验，而是设计更好的算法架构，学习自然界海量数据背后的高维流形。未来的发展圣杯，不仅仅是训练更大的模型，更要去**完善深度学习的数学理论**，打开这个“黑盒”，让 AI 真正成为人类探索未知边界的可靠伙伴。

**"AI for Math, Math for AI."**


[^1]: [Turing, A. M. (1950). Computing Machinery and Intelligence. _Mind_]([https://academic.oup.com/mind/article/LIX/236/433/986238](https://academic.oup.com/mind/article/LIX/236/433/986238))
[^2]: [Perceptrons: An Introduction to Computational Geometry (Minsky & Papert, 1969)](https://en.wikipedia.org/wiki/Perceptrons_\(book\)) 
[^3]: [Universal Approximation Theorem (Cybenko, 1989)](https://en.wikipedia.org/wiki/Universal_approximation_theorem)
[^4]: [Barron, A. R. (1993). Universal approximation bounds for superpositions of a sigmoidal function. _IEEE Transactions on Information Theory_, 39(3), 930-945.](https://ieeexplore.ieee.org/document/248452)
[^5]: [Deep Residual Learning for Image Recognition (ResNet, 2016)](https://arxiv.org/abs/1512.03385)
[^6]: [Hamilton–Jacobi–Bellman equation](https://en.wikipedia.org/wiki/Hamilton%E2%80%93Jacobi%E2%80%93Bellman_equation)
[^7]: [Vaswani, A., et al. (2017). Attention Is All You Need. _NIPS_.](https://arxiv.org/abs/1706.03762)
[^8]: [Introducing Brainiac Buddy, your AI-powered teaching assistant](https://newsen.pku.edu.cn/news_events/news/campus/13983.html)
[^9]: [Formal verification](https://en.wikipedia.org/wiki/Formal_verification)

## AI协作记录
<div style="font-size: small;">

`model: gemini3pro`

**Prompt 1:**

在笔记的第一部分，1956-1974年间的AI领域有哪些发展？可细分出几个流派？后文提到的神经网络是何时出现的？请你为我简要概括，使得人工智能发展简史部分的概念衔接更流畅，并提供相关资料补充。

**Answer:**

针对您提出的关于 **1956-1974年（即从达特茅斯会议到第一次AI寒冬）** 间历史细节的补充需求，这一时期通常被称为人工智能的 **“黄金年代”（The Golden Years）**。
在此期间，AI领域并未立刻陷入沉寂，而是经历了第一次爆发式的增长，形成了显著的 **流派之争**。关于您提到的神经网络出现时间，实际上它贯穿了这一时期，但经历了从高潮到低谷的过程。
以下是为您整理的回答，旨在让笔记的逻辑更顺畅：
##### Q1: 1956-1974年间的发展与流派？
这一时期是 **“符号主义” (Symbolism)** 占据绝对统治地位的时代，同时 **“连接主义” (Connectionism)** 也在早期进行了激烈的尝试。可以细分为两大主要流派：
- **符号主义 (Symbolism) —— 也就是当时的“主流 AI”**
    - **核心理念**：认为智能通过处理**符号**和**逻辑规则**来实现。人类思维被看作是一个物理符号系统。
    - **代表成就**：
        - **通用问题求解器 (GPS)**：Newell 和 Simon 开发，试图模拟人类解决问题的步骤。
        - **LISP 语言**：John McCarthy 发明，成为AI的标准编程语言。
        - **微世界 (Micro-worlds)**：如 Minsky 的积木世界，以及最早的聊天机器人 **ELIZA** (1966)。
    - **地位**：在这一阶段是绝对的主流，获得了DARPA的巨额资助。
- **连接主义 (Connectionism) —— 神经网络的早期形式**
    - **核心理念**：受神经科学启发，试图通过模拟大脑神经元连接（仿生学）来产生智能。
    - **代表成就**：**感知机 (Perceptron)**。
    - **结局**：在1969年被符号主义的大佬（Minsky）“从数学上判了死刑”，导致这一流派进入了长达十多年的冬眠。
##### Q2: 神经网络究竟是何时出现的？
神经网络的出现时间早于1956年，但在1956-1974年间经历了第一次大起大落。
- **萌芽 (1943)**：**McCulloch-Pitts 神经元模型**提出，这是最早的数学模型（属于笔记中的1.1节）。
- **诞生 (1957/1958)**：**Frank Rosenblatt** 发明了 **感知机 (Perceptron)**，这是第一个真正可以“学习”的神经网络硬件，引发了第一次神经网络热潮。
- **受挫 (1969)**：Minsky 和 Papert 出版《Perceptrons》一书，证明单层感知机无法解决 XOR 问题，直接导致了连接主义的衰落（即笔记中提到的1.2节的开端）。
##### 补充资料 (Supplementary Materials)
如果您需要进一步拓展，可以参考以下关键文献和概念：
1. **关于流派**：除了符号主义和连接主义，后来（80年代后）还兴起了**行为主义 (Actionism)**，以 Rodney Brooks 的机器昆虫为代表，主张“感知-行动”，无需复杂的内部模型。这三者并称为AI的三大流派。
2. **关于感知机**：Frank Rosenblatt 当时制造的 Mark I 感知机并不是软件，而是一台充满电线的庞大硬件机器，现存于史密森尼博物馆。
3. **关键论文**：
    - _McCulloch, W. S., & Pitts, W. (1943)._ (神经网络的概念起源)
    - _Rosenblatt, F. (1958). The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain._ (神经网络的第一次工程实现)
    - _Turing, A. M. (1950)._ (符号主义的思想源头之一）
希望这些补充能帮助您更流畅地衔接笔记中的历史脉络。
[人工智能历史：从图灵测试到深度学习](https://www.youtube.com/watch?v=6fXNiJXUheI)
这个视频详细回顾了神经网络从1943年McCulloch-Pitts模型到现代深度学习的发展历程，特别涵盖了您关心的1956-1974年间感知机的兴起与Minsky造成的寒冬。

**Prompt 2:**

AI在数学定理的形式化证明上有什么优势？

**Answer:**

AI在数学定理的形式化证明（Formal Verification）上的优势主要体现在以下几个核心方面。这些优势恰恰解决了传统形式化证明中“门槛高”和“效率低”的痛点。
##### 1. “自动形式化” (Autoformalization)：降低翻译门槛
这是AI目前最大的贡献之一。
- **痛点**：传统的形式化证明需要数学家学习复杂的编程语言（如 Lean, Coq, Isabelle）。将教科书上的“自然语言数学”翻译成“计算机代码”是一项极其耗时且枯燥的工作。
- **AI优势**：AI（特别是基于Transformer的大语言模型）可以充当**翻译官**。它能够理解自然语言描述的数学定理，并将其转化为形式化系统（如Lean）可执行的代码。
    - _笔记关联_：对应笔记 4.2 节提到的“将自然语言证明转化为计算机可验证的代码”。
    - _意义_：这极大地降低了数学家使用形式化工具的门槛，让不懂代码的数学家也能参与到形式化证明中。
##### 2. 提供数学“直觉” (Intuition) 以剪枝搜索空间
形式化证明本质上是一个在一个巨大的逻辑空间中搜索路径的过程。
- **痛点**：每一步推导都有无数种可能的变形或引理调用，传统的自动化工具（如ATP，自动定理证明器）往往只能进行暴力搜索，容易陷入组合爆炸。
- **AI优势**：AI 通过学习海量的数学文献，具备了类似人类的**“直觉”**。它能预测哪一步推导更有可能通向证明的终点，从而极大地缩小搜索范围。
    - _笔记关联_：对应附录第3点提到的 AlphaGeometry，“AI不再是暴力搜索，而是像人类一样学会了‘添加辅助线’”。
    - _机制_：策略网络（Policy Network）指导证明方向，价值网络（Value Network）评估当前状态的好坏。
##### 3. 填补“显而易见”的细节 (Tactic Generation)
- **痛点**：在人类数学证明中，我们常说“显然易得”（trivial），但在形式化证明中，这些“显然”的步骤必须被极其严谨地写出，哪怕是简单的算术运算或不等式缩放。
- **AI优势**：AI 非常擅长填充这些繁琐的中间步骤。大模型可以自动生成这些“胶水代码”（Tactics），让人类数学家只需关注证明的高层逻辑（High-level Logic），而将底层的繁琐验证交给AI。
##### 4. 超越人类认知的复杂验证
- **痛点**：现代数学的一些证明已经复杂到单个数学家难以完全验证的程度（例如开普勒猜想的证明，或某些极高维度的代数几何问题）。
- **AI优势**：AI 不会疲劳，且记忆力无限。一旦证明被形式化，计算机内核（Kernel）保证了逻辑的绝对正确性，消除了人为疏忽导致的逻辑漏洞。
    - _笔记关联_：对应笔记 4.1 节提到的“复杂证明的验证耗时耗力，超出个体认知极限”。

在形式化证明中，**AI 是“向导”，形式化系统（如Lean）是“法官”**。 AI 负责用直觉快速找到可能的路径（生成证明），而形式化系统负责严格核查这条路径是否正确（验证证明）。这种结合完美解决了大模型容易产生“幻觉”的问题——因为如果AI“胡说八道”，形式化系统的编译器会直接报错，无法通过验证。

**Prompt 3:**

请你试着分别预判人工智能未来最有希望和最没有希望的三条发展方向，并结合历史发展和技术现状等方面，给出你的判断理由。

**Answer:**

以下是我对未来AI发展方向的预判。这些判断基于一个核心逻辑：**历史总是螺旋上升的，未来的突破往往在于解决当前范式的致命缺陷。**
##### 一、 最有希望的三条发展方向 (The Most Promising)
这三个方向都致力于解决当前深度学习的“不可解释性”、“逻辑缺陷”和“算力依赖”问题。
**AI for Science (AI4S) —— 从“逼近”到“求解”**
- **预判理由**：这是数学与AI结合最紧密的领域，也是打破传统科学计算瓶颈的唯一解。
- **历史与技术支撑**    
    - **打破维数灾难**：历史上的传统数值计算（如有限元法）在高维空间会遭遇计算量指数级爆炸。Barron在1993年证明了神经网络逼近高维函数的误差与维度无关，这为AI解决复杂的科学方程提供了数学合法性。
    - **现实需求**：从AlphaFold预测蛋白质结构到GraphCast预测天气，AI已经证明了它处理海量数据背后高维流形的能力。未来的方向是将单纯的数据拟合（Data-Driven）进化为融合物理定律的神经算子（Neural Operators），直接求解复杂的偏微分方程。
**神经符号人工智能 (Neuro-Symbolic AI) —— “直觉”与“逻辑”的联姻**
- **预判理由**：解决大模型“幻觉”和逻辑推理不可靠的必经之路。
- **历史与技术支撑**：
    - **历史教训**：AI历史经历了“符号主义”（强逻辑、弱学习，1956-1980s）和“连接主义”（强学习、弱逻辑，2012-Present）的钟摆。现在的纯大模型（连接主义）虽然能进行“Next Token Prediction”，但缺乏严谨的逻辑推理，常出现一本正经胡说八道的情况。
    - **未来趋势**：未来的系统将融合两者——用神经网络提供“直觉”和“感知”（如AlphaGeometry中的添加辅助线），用符号系统（如形式化证明语言Lean）来进行严谨的逻辑验证。这能让AI既具备创造力，又具备数学上的可信度。
**可解释性数学理论 (White-box Theory) —— 打开“黑盒”**
- **预判理由**：工程应用跑得太快，基础理论必须跟上，否则发展将由“科学”退化为“炼丹术”。
- **历史与技术支撑**：
    - **现状痛点**：目前的深度学习本质上是一个“黑盒”，我们知道它好用，但不知道它**为什么**好用，缺乏完备的数学理论支撑。
    - **发展动力**：董彬教授指出，未来的圣杯在于“完善深度学习的数学理论”。只有建立像物理学那样的理论大厦（比如解释大模型的涌现机制、泛化边界），我们才能设计出更高效、更鲁棒的架构，而不是盲目地堆砌参数。
##### 二、 最没有希望的三条发展方向 (The Least Promising)
这三个方向往往是当前的热门泡沫，或者是历史上已经被证伪的路径的简单重复。
**纯粹的暴力模型堆砌 (Model-Centric Only)**
- **预判理由**：边际效应递减，且受到物理定律（能耗）和硬件的硬约束。
- **历史与技术支撑**：
    - **算力瓶颈**：课程中提到了GPU算力与框架的“卡脖子”风险。单纯追求参数量的指数级增长（从千亿到万亿再到千万亿），对算力和电力的消耗是不可持续的。
    - **范式转移**：现在的趋势已经开始从“Model-Centric”（盲目调优模型结构）转向“Data-Centric”（关注数据质量）。未来的模型需要更“聪明”，而不是更“胖”。
**违背物理规律的纯数据驱动模拟 (Naive Data-Driven Simulation)**
- **预判理由**：在严肃的工程与科学领域，不守恒的预测毫无价值。
- **历史与技术支撑**：
    - **可靠性问题**：现在的生成式AI可以生成非常逼真的流体图片，但如果它生成的流体不满足质量守恒或动量守恒，那么它在航空航天或桥梁设计中就是灾难性的。
    - **死胡同**：如果不引入物理方程作为约束（如PINNs的做法），仅靠拟合观测数据的“黑盒”模式在科学发现中是走不远的，因为它无法泛化到训练数据之外的物理场景。
**传统的专家系统复辟 (Neo-Expert Systems)**
- **预判理由**：历史已经证明，试图穷举人类规则来定义智能是行不通的。
- **历史与技术支撑**：
    - **历史教训**：1980-1987年间，专家系统试图通过硬编码人类规则来模拟智能，但因为缺乏学习能力和可拓展性，导致了第二次AI寒冬。
    - **逻辑死结**：现实世界的规则太复杂且充满模糊性，无法用有限的 `if-then` 规则库完全覆盖。任何试图退回到“手工编写规则”来解决通用智能问题的尝试，大概率重蹈覆辙。

**总结**： 未来的赢家属于那些能将**“神经网络的强拟合能力”**与**“数学/物理的严谨性”**结合起来的方向；而那些试图通过**蛮力堆砌算力**或**忽视物理法则**的尝试，终将撞上天花板。
</div>
